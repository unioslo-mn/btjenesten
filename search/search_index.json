{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"En virtuell h\u00e5ndsrekning fra B-tjenesten Studentene i B-tjenesten (beregningstjenesten) ved Kjemisk institutt, UiO bidrar til b\u00e5de godt milj\u00f8, veiledning og nyskapende l\u00e6ringsressurser. Ved siden av v\u00e5r ukentlige veiledningstjeneste p\u00e5 seminarrom Curie er vi involvert i programmering- og beregningsorientert innovasjon i undervisningen ved instituttet. Vi utvikler blant annet undervisningsmateriell, dokumentasjon, og kan bist\u00e5 med \u00e5 tilrettelegge eksisterende kode for undervisning. For \u00e5 im\u00f8tekomme en verden hvor nyvinninger br\u00e5tt kan skape nye behov og muligheter trenger vi dessuten ofte ny kode og tekniske l\u00f8sninger for undervisningen. V\u00e5re bidrag til dette finner du i v\u00e5r utstrakte virtuelle h\u00e5nd; Python-modulen btjenesten . Du kan installere oss med pip install btjenesten ...og importere oss med import btjenesten P\u00e5 disse sidene finner du dokumentasjon for v\u00e5r modul, samt litt informasjon om v\u00e5re aktiviteter. For mer informasjon, kom innom B-tjenesten p\u00e5 v\u00e5re veilednings\u00f8kter, torsdager 16.00-18.00 p\u00e5 seminarrom Curie ved Kjemisk institutt, UiO. Du finner oss ogs\u00e5 p\u00e5 Facebook: B-tjenesten","title":"Hjem"},{"location":"#en-virtuell-handsrekning-fra-b-tjenesten","text":"Studentene i B-tjenesten (beregningstjenesten) ved Kjemisk institutt, UiO bidrar til b\u00e5de godt milj\u00f8, veiledning og nyskapende l\u00e6ringsressurser. Ved siden av v\u00e5r ukentlige veiledningstjeneste p\u00e5 seminarrom Curie er vi involvert i programmering- og beregningsorientert innovasjon i undervisningen ved instituttet. Vi utvikler blant annet undervisningsmateriell, dokumentasjon, og kan bist\u00e5 med \u00e5 tilrettelegge eksisterende kode for undervisning. For \u00e5 im\u00f8tekomme en verden hvor nyvinninger br\u00e5tt kan skape nye behov og muligheter trenger vi dessuten ofte ny kode og tekniske l\u00f8sninger for undervisningen. V\u00e5re bidrag til dette finner du i v\u00e5r utstrakte virtuelle h\u00e5nd; Python-modulen btjenesten . Du kan installere oss med pip install btjenesten ...og importere oss med import btjenesten P\u00e5 disse sidene finner du dokumentasjon for v\u00e5r modul, samt litt informasjon om v\u00e5re aktiviteter. For mer informasjon, kom innom B-tjenesten p\u00e5 v\u00e5re veilednings\u00f8kter, torsdager 16.00-18.00 p\u00e5 seminarrom Curie ved Kjemisk institutt, UiO. Du finner oss ogs\u00e5 p\u00e5 Facebook: B-tjenesten","title":"En virtuell h\u00e5ndsrekning fra B-tjenesten"},{"location":"analysis-reference/","text":"Analysis Tools for analysing GPR predictions. Author: Audun Skau Hansen choose_n_most_distant ( all_x , n ) choose n measurements such that the total distance between the measurements is at a maximum Author: Audun Source code in btjenesten/analysis.py 183 184 185 186 187 188 189 190 191 192 def choose_n_most_distant ( all_x , n ): \"\"\" choose n measurements such that the total distance between the measurements is at a maximum Author: Audun \"\"\" d = np . sum (( all_x [:, None ] - all_x [ None , :]) ** 2 , axis = 2 ) total_d = np . sum ( d , axis = 0 ) return np . argsort ( total_d )[ - n :] data_projection ( regressor , axes = [ 0 ], resolution = 20 , center = None ) Project high-dimensional regressor predictions onto smaller spaces. Author: Audun Skau Hansen Arguments regressor = a gpr regressor axies = indices of axis to sweep over resolution = resolution of sweeps along all axes center = (optional) set center point of sweep (default is middle of the region) Examples x, y = data_projection(regressor, axes = [0]) -> all axes except 0 are kept fixed at the mean values (or center values), while 0 is allowed to vary inside the fitting region. plot(x[0], y) x, y = data_projection(regressor, axes = [1,2]) -> all axes except 1 and 2 are fixed to mean values (or center values) while 1 and 2 are allowed to map a surface in the fitting region. contourf(x[0], x[1], y) Source code in btjenesten/analysis.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def data_projection ( regressor , axes = [ 0 ], resolution = 20 , center = None ): \"\"\" Project high-dimensional regressor predictions onto smaller spaces. Author: Audun Skau Hansen Arguments === regressor = a gpr regressor axies = indices of axis to sweep over resolution = resolution of sweeps along all axes center = (optional) set center point of sweep (default is middle of the region) Examples === x, y = data_projection(regressor, axes = [0]) -> all axes except 0 are kept fixed at the mean values (or center values), while 0 is allowed to vary inside the fitting region. plot(x[0], y) x, y = data_projection(regressor, axes = [1,2]) -> all axes except 1 and 2 are fixed to mean values (or center values) while 1 and 2 are allowed to map a surface in the fitting region. contourf(x[0], x[1], y) \"\"\" # extract fitting regions (only internal datapoints will be predicted) mean = np . mean ( regressor . recover ( regressor . training_data_X ), axis = 0 ) if center is not None : mean = np . array ( center ) #print(regressor.recover(regressor.training_data_X)) bound = np . max ( regressor . recover ( regressor . training_data_X ), axis = 0 ) - np . min ( regressor . recover ( regressor . training_data_X ), axis = 0 ) #print(mean, bound) lower_bound = mean - bound upper_bound = mean + bound # create a grid wherein the datapoints are interpolated grid = [] for i in range ( len ( lower_bound )): grid . append ( np . linspace ( - bound [ i ], bound [ i ], resolution )) #if center is None: # center = np.zeros(len(mean), dtype = float) mgrid = list ( mean ) #[0 for i in range(len(center))] for i in range ( len ( axes )): mgrid [ axes [ i ]] = grid [ axes [ i ]] + mean [ axes [ i ]] prediction_grid = np . array ( np . meshgrid ( * mgrid )) . reshape ( len ( mean ), - 1 ) # return prediction to user x = [] # list to contain the relevant grid-points for i in range ( len ( axes )): x . append ( mgrid [ axes [ i ]]) return x , regressor . predict ( prediction_grid . T ) . reshape ([ resolution for i in range ( len ( axes ))]) html_table ( values , columns = None , rows = None ) Simple HTML table generator Author: Audun Skau Hansen Source code in btjenesten/analysis.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def html_table ( values , columns = None , rows = None ): \"\"\" Simple HTML table generator Author: Audun Skau Hansen \"\"\" ret = \"\"\"<table> \\n \"\"\" nr = values . shape [ 0 ] nc = values . shape [ 1 ] if columns is not None : ret += \"<tr> \\n \" ret += \"<th></th> \\n \" for c in range ( len ( columns )): ret += \"<th> %s </th> \\n \" % columns [ c ] ret += \"</tr> \\n \" for r in range ( nr ): ret += \"<tr> \\n \" if columns is not None : if rows is not None : ret += \"<th> %s </th> \\n \" % rows [ r ] else : ret += \"<th></th> \\n \" else : if rows is not None : ret += \"<th> %s </th> \\n \" % rows [ r ] else : ret += \"<th></th> \\n \" for c in range ( nc ): ret += \"<th> %s </th> \\n \" % values [ r , c ] ret += \"</tr> \\n \" ret += \"\"\"</table> \\n \"\"\" return HTML ( ret ) one_dimensional_plots ( x_data , proposed_new_x , reg , y_labels , x_labels ) Author: Ayla S. Coder Source code in btjenesten/analysis.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def one_dimensional_plots ( x_data , proposed_new_x , reg , y_labels , x_labels ): \"\"\" Author: Ayla S. Coder \"\"\" import matplotlib.pyplot as plt # Where y_labels and x_labels are arrays of strings plt . figure ( figsize = ( 8 , 5 )) # Sets the figure size rows = int ( len ( x_data [ 0 ]) / 3 ) # The amount of rows of plots in the figure dimensions = len ( x_data [ 0 ]) # The amount of dimensions evaluated. # For loop to set up each subplot for i in range ( dimensions ): # This function is documented on Btjeneste website. If you're curious, run the command help(bt.analysis.data_projection) x , y = data_projection ( reg , axes = [ i ], center = proposed_new_x , resolution = 100 ) # Sets up the i-th subplot ax = plt . subplot ( rows , dimensions , i + 1 ) ax . set_ylabel ( y_labels [ i ], fontsize = 15 ) ax . set_xlabel ( x_labels [ i ], fontsize = 15 ) ax . grid () # What is plotted in each plot: ax . plot ( x [ 0 ], y ) plt . show () parameter_optimization ( x_data , y_data , training_fraction = 0.8 , normalize_y = True , params = None , training_subset = None ) Draft for a parameter optimization scheme Author: Audun Skau Hansen Takes as input the dataset (x_data, y_data) and the fraction (a float in the interval 0.0 - 1.0) of datapoints to use as training data. Source code in btjenesten/analysis.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def parameter_optimization ( x_data , y_data , training_fraction = 0.8 , normalize_y = True , params = None , training_subset = None ): \"\"\" Draft for a parameter optimization scheme Author: Audun Skau Hansen Takes as input the dataset (x_data, y_data) and the fraction (a float in the interval 0.0 - 1.0) of datapoints to use as training data. \"\"\" n = int ( training_fraction * x_data . shape [ 0 ]) # special first iteration if params is None : params = np . ones ( x_data . shape [ 1 ]) *- 2.0 #*0.001 #training_subset = np.random.choice(x_data.shape[0], n, replace = False) if training_subset is None : training_subset = np . ones ( x_data . shape [ 0 ], dtype = bool ) training_subset [ n :] = False else : if len ( training_subset ) < len ( y_data ): # assume index element array ts = np . zeros ( len ( y_data ), dtype = bool ) ts [ training_subset ] = True training_subset = ts #print(training_subset) #print(x_data[training_subset]) #print(training_subset) y_data_n = y_data * 1 if normalize_y : y_data_n *= y_data_n . max () **- 1 def residual ( params , x_data = x_data , y_data = y_data_n , training_subset = training_subset ): test_subset = np . ones ( x_data . shape [ 0 ], dtype = bool ) test_subset [ training_subset ] = False regressor = Regressor ( x_data [ training_subset ] , y_data [ training_subset ]) regressor . params = 10 ** params energy = np . sum (( regressor . predict ( x_data [ test_subset ]) - y_data [ test_subset ]) ** 2 ) return energy ret = minimize ( residual , params ) #print(ret) return 10 ** ret [ \"x\" ] parameter_tuner_3d ( all_x , all_y , n ) Interactive (widget for Jupyter environments) parameter tuner for the gpr module Authors: Audun Skau Hansen and Ayla S. Coder Source code in btjenesten/analysis.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def parameter_tuner_3d ( all_x , all_y , n ): \"\"\" Interactive (widget for Jupyter environments) parameter tuner for the gpr module Authors: Audun Skau Hansen and Ayla S. Coder \"\"\" import matplotlib.pyplot as plt from matplotlib.widgets import Slider , Button training_x = all_x [ n ] training_y = all_y [ n ] regressor = Regressor ( training_x , training_y ) # The parametrized function to be plotted def f ( params1 , params2 , params3 ): regressor . params = 10 ** np . array ([ params1 , params2 , params3 ]) return regressor . predict ( all_x ) # Create the figure and the line that we will manipulate fig , ax = plt . subplots () plt . plot ( np . arange ( len ( all_y ))[ n ], all_y [ n ], \"o\" , markersize = 10 , label = \"training data\" , color = ( 0 , 0 , .5 )) plt . plot ( all_y , \"o\" , label = \"true values\" , color = ( .9 , .2 , .4 )) plt . legend () line , = plt . plot ( f ( 1 , 1 , 1 ), \".-\" , lw = 1 , color = ( .9 , .9 , .2 )) ax . set_xlabel ( 'Time [s]' ) # adjust the main plot to make room for the sliders plt . subplots_adjust ( left = 0.4 , bottom = 0.25 ) \"\"\" # Make a horizontal slider to control the frequency. axfreq = plt.axes([0.25, 0.1, 0.65, 0.03]) freq_slider = Slider( ax=axfreq, label='Frequency [Hz]', valmin=0.1, valmax=30, valinit=init_frequency, ) \"\"\" # Make a vertically oriented slider to control the amplitude param1 = plt . axes ([ 0.1 , 0.3 , 0.02 , 0.5 ]) param_slider1 = Slider ( ax = param1 , label = \"log(P1)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # Make a vertically oriented slider to control the amplitude param2 = plt . axes ([ 0.2 , 0.3 , 0.02 , 0.5 ]) param_slider2 = Slider ( ax = param2 , label = \"log(P2)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # Make a vertically oriented slider to control the amplitude param3 = plt . axes ([ 0.3 , 0.3 , 0.02 , 0.5 ]) param_slider3 = Slider ( ax = param3 , label = \"log(P3)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # The function to be called anytime a slider's value changes def update ( val ): line . set_ydata ( f ( param_slider1 . val , param_slider2 . val , param_slider3 . val )) fig . canvas . draw_idle () # register the update function with each slider #freq_slider.on_changed(update) param_slider1 . on_changed ( update ) param_slider2 . on_changed ( update ) param_slider3 . on_changed ( update ) # Create a `matplotlib.widgets.Button` to reset the sliders to initial values. resetax = plt . axes ([ 0.8 , 0.025 , 0.1 , 0.04 ]) button = Button ( resetax , 'Reset' , hovercolor = '0.975' ) def reset ( event ): param_slider1 . reset () param_slider2 . reset () param_slider3 . reset () button . on_clicked ( reset ) plt . show () remove_redundancy ( x_train , y_train , tol = 1e-07 ) extract unique columns of x_train (and corresponding elements in y_train) Author: Audun Skau Hansen Source code in btjenesten/analysis.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def remove_redundancy ( x_train , y_train , tol = 10e-8 ): \"\"\" extract unique columns of x_train (and corresponding elements in y_train) Author: Audun Skau Hansen \"\"\" ns = x_train . shape [ 0 ] #number of measurements # compute the \"euclidean distance\" d = np . sum (( x_train [:, None ] - x_train [ None , :]) ** 2 , axis = 2 ) active = np . ones ( ns , dtype = bool ) unique_training_x = [] unique_training_y = [] for i in range ( ns ): distances = d [ i ] da = distances [ active ] ia = np . arange ( ns )[ active ] elms = ia [ da < tol ] active [ elms ] = False if len ( elms ) > 0 : unique_training_x . append ( x_train [ elms [ 0 ]]) unique_training_y . append ( np . mean ( y_train [ elms ], axis = 0 )) return np . array ( unique_training_x ), np . array ( unique_training_y ) show_3d_sample_box ( all_x ) a \"hack\" for showing the measurement setup in 3D using the bubblebox module / evince Author: Audun Source code in btjenesten/analysis.py 171 172 173 174 175 176 177 178 179 180 181 def show_3d_sample_box ( all_x ): \"\"\" a \"hack\" for showing the measurement setup in 3D using the bubblebox module / evince Author: Audun \"\"\" import bubblebox as bb b = bb . mdbox ( n_bubbles = all_x . shape [ 0 ], size = ( 4 , 4 , 4 )) pos = all_x - np . min ( all_x , axis = 0 )[ None , :] pos = pos * np . max ( pos , axis = 0 )[ None , :] **- 1 b . pos = 8 * ( pos . T - np . ones ( 3 )[:, None ] * .5 ) return b","title":"analysis"},{"location":"analysis-reference/#analysis","text":"Tools for analysing GPR predictions. Author: Audun Skau Hansen","title":"Analysis"},{"location":"analysis-reference/#btjenesten.analysis.choose_n_most_distant","text":"choose n measurements such that the total distance between the measurements is at a maximum Author: Audun Source code in btjenesten/analysis.py 183 184 185 186 187 188 189 190 191 192 def choose_n_most_distant ( all_x , n ): \"\"\" choose n measurements such that the total distance between the measurements is at a maximum Author: Audun \"\"\" d = np . sum (( all_x [:, None ] - all_x [ None , :]) ** 2 , axis = 2 ) total_d = np . sum ( d , axis = 0 ) return np . argsort ( total_d )[ - n :]","title":"choose_n_most_distant()"},{"location":"analysis-reference/#btjenesten.analysis.data_projection","text":"Project high-dimensional regressor predictions onto smaller spaces. Author: Audun Skau Hansen","title":"data_projection()"},{"location":"analysis-reference/#btjenesten.analysis.data_projection--arguments","text":"regressor = a gpr regressor axies = indices of axis to sweep over resolution = resolution of sweeps along all axes center = (optional) set center point of sweep (default is middle of the region)","title":"Arguments"},{"location":"analysis-reference/#btjenesten.analysis.data_projection--examples","text":"x, y = data_projection(regressor, axes = [0]) -> all axes except 0 are kept fixed at the mean values (or center values), while 0 is allowed to vary inside the fitting region. plot(x[0], y) x, y = data_projection(regressor, axes = [1,2]) -> all axes except 1 and 2 are fixed to mean values (or center values) while 1 and 2 are allowed to map a surface in the fitting region. contourf(x[0], x[1], y) Source code in btjenesten/analysis.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def data_projection ( regressor , axes = [ 0 ], resolution = 20 , center = None ): \"\"\" Project high-dimensional regressor predictions onto smaller spaces. Author: Audun Skau Hansen Arguments === regressor = a gpr regressor axies = indices of axis to sweep over resolution = resolution of sweeps along all axes center = (optional) set center point of sweep (default is middle of the region) Examples === x, y = data_projection(regressor, axes = [0]) -> all axes except 0 are kept fixed at the mean values (or center values), while 0 is allowed to vary inside the fitting region. plot(x[0], y) x, y = data_projection(regressor, axes = [1,2]) -> all axes except 1 and 2 are fixed to mean values (or center values) while 1 and 2 are allowed to map a surface in the fitting region. contourf(x[0], x[1], y) \"\"\" # extract fitting regions (only internal datapoints will be predicted) mean = np . mean ( regressor . recover ( regressor . training_data_X ), axis = 0 ) if center is not None : mean = np . array ( center ) #print(regressor.recover(regressor.training_data_X)) bound = np . max ( regressor . recover ( regressor . training_data_X ), axis = 0 ) - np . min ( regressor . recover ( regressor . training_data_X ), axis = 0 ) #print(mean, bound) lower_bound = mean - bound upper_bound = mean + bound # create a grid wherein the datapoints are interpolated grid = [] for i in range ( len ( lower_bound )): grid . append ( np . linspace ( - bound [ i ], bound [ i ], resolution )) #if center is None: # center = np.zeros(len(mean), dtype = float) mgrid = list ( mean ) #[0 for i in range(len(center))] for i in range ( len ( axes )): mgrid [ axes [ i ]] = grid [ axes [ i ]] + mean [ axes [ i ]] prediction_grid = np . array ( np . meshgrid ( * mgrid )) . reshape ( len ( mean ), - 1 ) # return prediction to user x = [] # list to contain the relevant grid-points for i in range ( len ( axes )): x . append ( mgrid [ axes [ i ]]) return x , regressor . predict ( prediction_grid . T ) . reshape ([ resolution for i in range ( len ( axes ))])","title":"Examples"},{"location":"analysis-reference/#btjenesten.analysis.html_table","text":"Simple HTML table generator Author: Audun Skau Hansen Source code in btjenesten/analysis.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def html_table ( values , columns = None , rows = None ): \"\"\" Simple HTML table generator Author: Audun Skau Hansen \"\"\" ret = \"\"\"<table> \\n \"\"\" nr = values . shape [ 0 ] nc = values . shape [ 1 ] if columns is not None : ret += \"<tr> \\n \" ret += \"<th></th> \\n \" for c in range ( len ( columns )): ret += \"<th> %s </th> \\n \" % columns [ c ] ret += \"</tr> \\n \" for r in range ( nr ): ret += \"<tr> \\n \" if columns is not None : if rows is not None : ret += \"<th> %s </th> \\n \" % rows [ r ] else : ret += \"<th></th> \\n \" else : if rows is not None : ret += \"<th> %s </th> \\n \" % rows [ r ] else : ret += \"<th></th> \\n \" for c in range ( nc ): ret += \"<th> %s </th> \\n \" % values [ r , c ] ret += \"</tr> \\n \" ret += \"\"\"</table> \\n \"\"\" return HTML ( ret )","title":"html_table()"},{"location":"analysis-reference/#btjenesten.analysis.one_dimensional_plots","text":"Author: Ayla S. Coder Source code in btjenesten/analysis.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def one_dimensional_plots ( x_data , proposed_new_x , reg , y_labels , x_labels ): \"\"\" Author: Ayla S. Coder \"\"\" import matplotlib.pyplot as plt # Where y_labels and x_labels are arrays of strings plt . figure ( figsize = ( 8 , 5 )) # Sets the figure size rows = int ( len ( x_data [ 0 ]) / 3 ) # The amount of rows of plots in the figure dimensions = len ( x_data [ 0 ]) # The amount of dimensions evaluated. # For loop to set up each subplot for i in range ( dimensions ): # This function is documented on Btjeneste website. If you're curious, run the command help(bt.analysis.data_projection) x , y = data_projection ( reg , axes = [ i ], center = proposed_new_x , resolution = 100 ) # Sets up the i-th subplot ax = plt . subplot ( rows , dimensions , i + 1 ) ax . set_ylabel ( y_labels [ i ], fontsize = 15 ) ax . set_xlabel ( x_labels [ i ], fontsize = 15 ) ax . grid () # What is plotted in each plot: ax . plot ( x [ 0 ], y ) plt . show ()","title":"one_dimensional_plots()"},{"location":"analysis-reference/#btjenesten.analysis.parameter_optimization","text":"Draft for a parameter optimization scheme Author: Audun Skau Hansen Takes as input the dataset (x_data, y_data) and the fraction (a float in the interval 0.0 - 1.0) of datapoints to use as training data. Source code in btjenesten/analysis.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def parameter_optimization ( x_data , y_data , training_fraction = 0.8 , normalize_y = True , params = None , training_subset = None ): \"\"\" Draft for a parameter optimization scheme Author: Audun Skau Hansen Takes as input the dataset (x_data, y_data) and the fraction (a float in the interval 0.0 - 1.0) of datapoints to use as training data. \"\"\" n = int ( training_fraction * x_data . shape [ 0 ]) # special first iteration if params is None : params = np . ones ( x_data . shape [ 1 ]) *- 2.0 #*0.001 #training_subset = np.random.choice(x_data.shape[0], n, replace = False) if training_subset is None : training_subset = np . ones ( x_data . shape [ 0 ], dtype = bool ) training_subset [ n :] = False else : if len ( training_subset ) < len ( y_data ): # assume index element array ts = np . zeros ( len ( y_data ), dtype = bool ) ts [ training_subset ] = True training_subset = ts #print(training_subset) #print(x_data[training_subset]) #print(training_subset) y_data_n = y_data * 1 if normalize_y : y_data_n *= y_data_n . max () **- 1 def residual ( params , x_data = x_data , y_data = y_data_n , training_subset = training_subset ): test_subset = np . ones ( x_data . shape [ 0 ], dtype = bool ) test_subset [ training_subset ] = False regressor = Regressor ( x_data [ training_subset ] , y_data [ training_subset ]) regressor . params = 10 ** params energy = np . sum (( regressor . predict ( x_data [ test_subset ]) - y_data [ test_subset ]) ** 2 ) return energy ret = minimize ( residual , params ) #print(ret) return 10 ** ret [ \"x\" ]","title":"parameter_optimization()"},{"location":"analysis-reference/#btjenesten.analysis.parameter_tuner_3d","text":"Interactive (widget for Jupyter environments) parameter tuner for the gpr module Authors: Audun Skau Hansen and Ayla S. Coder Source code in btjenesten/analysis.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def parameter_tuner_3d ( all_x , all_y , n ): \"\"\" Interactive (widget for Jupyter environments) parameter tuner for the gpr module Authors: Audun Skau Hansen and Ayla S. Coder \"\"\" import matplotlib.pyplot as plt from matplotlib.widgets import Slider , Button training_x = all_x [ n ] training_y = all_y [ n ] regressor = Regressor ( training_x , training_y ) # The parametrized function to be plotted def f ( params1 , params2 , params3 ): regressor . params = 10 ** np . array ([ params1 , params2 , params3 ]) return regressor . predict ( all_x ) # Create the figure and the line that we will manipulate fig , ax = plt . subplots () plt . plot ( np . arange ( len ( all_y ))[ n ], all_y [ n ], \"o\" , markersize = 10 , label = \"training data\" , color = ( 0 , 0 , .5 )) plt . plot ( all_y , \"o\" , label = \"true values\" , color = ( .9 , .2 , .4 )) plt . legend () line , = plt . plot ( f ( 1 , 1 , 1 ), \".-\" , lw = 1 , color = ( .9 , .9 , .2 )) ax . set_xlabel ( 'Time [s]' ) # adjust the main plot to make room for the sliders plt . subplots_adjust ( left = 0.4 , bottom = 0.25 ) \"\"\" # Make a horizontal slider to control the frequency. axfreq = plt.axes([0.25, 0.1, 0.65, 0.03]) freq_slider = Slider( ax=axfreq, label='Frequency [Hz]', valmin=0.1, valmax=30, valinit=init_frequency, ) \"\"\" # Make a vertically oriented slider to control the amplitude param1 = plt . axes ([ 0.1 , 0.3 , 0.02 , 0.5 ]) param_slider1 = Slider ( ax = param1 , label = \"log(P1)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # Make a vertically oriented slider to control the amplitude param2 = plt . axes ([ 0.2 , 0.3 , 0.02 , 0.5 ]) param_slider2 = Slider ( ax = param2 , label = \"log(P2)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # Make a vertically oriented slider to control the amplitude param3 = plt . axes ([ 0.3 , 0.3 , 0.02 , 0.5 ]) param_slider3 = Slider ( ax = param3 , label = \"log(P3)\" , valmin =- 10 , valmax = 10 , valinit = 1.0 , orientation = \"vertical\" ) # The function to be called anytime a slider's value changes def update ( val ): line . set_ydata ( f ( param_slider1 . val , param_slider2 . val , param_slider3 . val )) fig . canvas . draw_idle () # register the update function with each slider #freq_slider.on_changed(update) param_slider1 . on_changed ( update ) param_slider2 . on_changed ( update ) param_slider3 . on_changed ( update ) # Create a `matplotlib.widgets.Button` to reset the sliders to initial values. resetax = plt . axes ([ 0.8 , 0.025 , 0.1 , 0.04 ]) button = Button ( resetax , 'Reset' , hovercolor = '0.975' ) def reset ( event ): param_slider1 . reset () param_slider2 . reset () param_slider3 . reset () button . on_clicked ( reset ) plt . show ()","title":"parameter_tuner_3d()"},{"location":"analysis-reference/#btjenesten.analysis.remove_redundancy","text":"extract unique columns of x_train (and corresponding elements in y_train) Author: Audun Skau Hansen Source code in btjenesten/analysis.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def remove_redundancy ( x_train , y_train , tol = 10e-8 ): \"\"\" extract unique columns of x_train (and corresponding elements in y_train) Author: Audun Skau Hansen \"\"\" ns = x_train . shape [ 0 ] #number of measurements # compute the \"euclidean distance\" d = np . sum (( x_train [:, None ] - x_train [ None , :]) ** 2 , axis = 2 ) active = np . ones ( ns , dtype = bool ) unique_training_x = [] unique_training_y = [] for i in range ( ns ): distances = d [ i ] da = distances [ active ] ia = np . arange ( ns )[ active ] elms = ia [ da < tol ] active [ elms ] = False if len ( elms ) > 0 : unique_training_x . append ( x_train [ elms [ 0 ]]) unique_training_y . append ( np . mean ( y_train [ elms ], axis = 0 )) return np . array ( unique_training_x ), np . array ( unique_training_y )","title":"remove_redundancy()"},{"location":"analysis-reference/#btjenesten.analysis.show_3d_sample_box","text":"a \"hack\" for showing the measurement setup in 3D using the bubblebox module / evince Author: Audun Source code in btjenesten/analysis.py 171 172 173 174 175 176 177 178 179 180 181 def show_3d_sample_box ( all_x ): \"\"\" a \"hack\" for showing the measurement setup in 3D using the bubblebox module / evince Author: Audun \"\"\" import bubblebox as bb b = bb . mdbox ( n_bubbles = all_x . shape [ 0 ], size = ( 4 , 4 , 4 )) pos = all_x - np . min ( all_x , axis = 0 )[ None , :] pos = pos * np . max ( pos , axis = 0 )[ None , :] **- 1 b . pos = 8 * ( pos . T - np . ones ( 3 )[:, None ] * .5 ) return b","title":"show_3d_sample_box()"},{"location":"bakgrunn/","text":"","title":"Bakgrunn"},{"location":"bbtools-reference/","text":"Box-Behnken Tools (bbtools) tablewidget Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022 Source code in btjenesten/bbtools.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class tablewidget (): \"\"\" Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022 \"\"\" def __init__ ( self , column_headers , row_headers ): self . tab = np . zeros (( len ( row_headers ) + 1 , len ( column_headers ) + 1 ), dtype = object ) #self.tab[1:,0] = row_headers #self.tab[0,1:] = column_headers self . row_headers = row_headers self . column_headers = column_headers self . tab [ 0 , 0 ] = widgets . Label ( \"\" ) for i in range ( len ( column_headers )): self . tab [ 0 , i + 1 ] = widgets . Label ( column_headers [ i ], align = \"right\" ) for j in range ( len ( row_headers )): self . tab [ j + 1 , 0 ] = widgets . Label ( row_headers [ j ], align = \"center\" ) self . items = [] for i in range ( len ( row_headers )): for j in range ( len ( column_headers )): self . tab [ i + 1 , j + 1 ] = widgets . BoundedFloatText ( value = 0 , min =- 1e15 , max = 1e15 , step = 0.1 , description = '' , disabled = False ) self . widget = widgets . GridBox ( list ( self . tab . ravel ()), layout = widgets . Layout ( grid_template_columns = \"repeat( %i , 100px)\" % ( len ( column_headers ) + 1 ))) def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ]) def _repr ( self ): \"\"\" Returns a latex-formatted string to display the mathematical expression of the basisfunction. \"\"\" return self . widget as_numpy_array () Returns the table (excluding headers) as a numpy array Source code in btjenesten/bbtools.py 47 48 49 50 51 52 53 54 55 def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret set_from_array ( input_array ) Set the table (excluding headers) from an input array Source code in btjenesten/bbtools.py 57 58 59 60 61 62 63 def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ]) bbdesign ( n_center = 3 , randomize = True , sheet = None ) Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments: Argument Description n_center number of samples in the center randomize whether or not to randomize the ordering (bool) sheet heet containing the min/max values of variables Source code in btjenesten/bbtools.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def bbdesign ( n_center = 3 , randomize = True , sheet = None ): \"\"\" Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | n_center | number of samples in the center | | randomize | whether or not to randomize the ordering (bool) | | sheet | heet containing the min/max values of variables | \"\"\" a = np . arange ( - 1 , 2 ) A = np . array ( np . meshgrid ( a , a , a )) . reshape ( 3 , - 1 ) . T A = np . concatenate ([ A [ np . sum ( A ** 2 , axis = 1 ) == 2 , :], np . zeros (( n_center , 3 ))]) ai = np . arange ( len ( A )) if randomize == True : # randomize run order np . random . shuffle ( ai ) if sheet is not None : # Transform coordinates tm = sheet . as_numpy_array () for i in range ( 3 ): A [:, i ] = interp1d ( np . linspace ( - 1 , 1 , 2 ), tm [ i ] )( A [:, i ]) #A = A.dot(tm) return A [ ai , :], ai bbsetup () Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 def bbsetup (): \"\"\" Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" global sheet arr = np . zeros (( 4 , 3 ), dtype = object ) arr [ 0 , 0 ] = \"\" arr [ 1 , 0 ] = \"Variable A\" arr [ 2 , 0 ] = \"Variable B\" arr [ 3 , 0 ] = \"Variable C\" arr [ 0 , 1 ] = \"Minimum\" arr [ 0 , 2 ] = \"Maximum\" arr [ 1 :, 1 ] = - 1 arr [ 1 :, 2 ] = 1 sheet = from_array ( arr ) sheet . column_headers = False sheet . row_headers = False return sheet bbsheet ( sheet ) Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments sheet = setup from bbsetup Source code in btjenesten/bbtools.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def bbsheet ( sheet ): \"\"\" Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments sheet = setup from bbsetup \"\"\" bd , ai = bbdesign ( sheet = sheet ) #global bb_sheet #sh = sheet.as_numpy_array() #arr = np.zeros(sh.shape + np.array([1,2]), dtype = object) #print(\"sh\") #print(sh) #print(bd) column_headers = [ \"Run\" , sheet . row_headers [ 0 ], sheet . row_headers [ 1 ], sheet . row_headers [ 2 ], \"Result\" ] row_headers = [ \"\" for i in range ( bd . shape [ 0 ])] bb_widget = tablewidget ( column_headers , row_headers ) arr = np . zeros ( ( len ( row_headers ), len ( column_headers )), dtype = float ) arr [:, 0 ] = np . arange ( len (( row_headers ))) + 1 arr [:, 0 ] = ai + 1 arr [:, 1 : 4 ] = bd bb_widget . set_from_array ( arr ) return bb_widget minitable ( titles , values , sheet ) Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO Keyword arguments: Argument Description titles default variables values coefficients sheet the Box-Behnken sheet Source code in btjenesten/bbtools.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def minitable ( titles , values , sheet ): \"\"\" Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | titles | default variables | | values | coefficients | | sheet | the Box-Behnken sheet | \"\"\" arr = np . zeros (( len ( titles ), 2 ), dtype = object ) arr [:, 0 ] = relabel_defaults ( titles , sheet ) arr [:, 1 ] = values return from_array ( arr ) relabel_defaults ( titles , new_names ) Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py 311 312 313 314 315 316 317 318 319 320 321 def relabel_defaults ( titles , new_names ): \"\"\" Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" #new_names = to_array(sheet)[1:4,0] new_titles = [] for i in titles : new_titles . append ( i . replace ( \"x0\" , new_names [ 0 ]) . replace ( \"x1\" , new_names [ 1 ]) . replace ( \"x2\" , new_names [ 2 ]) ) return new_titles visualize_surfaces ( bbwidget , Nx = 30 ) Visualize response surfaces the regressor model Author : Audun Skau Hansen, Department of Chemistry, UiO (2022) Keyword arguments: Argument Description sheet Box-Benhken data sheet regressor sklearn LinearRegression instance Nx mesh resolution along each axis Source code in btjenesten/bbtools.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def visualize_surfaces ( bbwidget , Nx = 30 ): \"\"\" Visualize response surfaces the regressor model **Author**: Audun Skau Hansen, Department of Chemistry, UiO (2022) ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | sheet | Box-Benhken data sheet | | regressor | sklearn LinearRegression instance | | Nx | mesh resolution along each axis | \"\"\" data = bbwidget . as_numpy_array () bounds = np . zeros (( 3 , 2 ), dtype = float ) bounds [:, 0 ] = np . min ( data [:, 1 : 4 ], axis = 0 ) bounds [:, 1 ] = np . max ( data [:, 1 : 4 ], axis = 0 ) #if regressor is None: # crop data from the sheet above X_train = data [:, 1 : 4 ] y_train = data [:, 4 ] #print(X_train, y_train) # perform a second order polynomial fit # (linear in the matrix elements) degree = 2 # second order poly = PolynomialFeatures ( degree ) # these are the matrix elements regressor = make_pipeline ( poly , LinearRegression ()) #set up the regressor regressor . fit ( X_train , y_train ) # fit the model # first, we extract all relevant information coefficients = regressor . steps [ 1 ][ 1 ] . coef_ names = poly . get_feature_names () predicted = regressor . predict ( X_train ) measured = y_train score = regressor . score ( X_train , y_train ) # we then compute and tabulate various statistics squared_error = ( predicted - measured ) ** 2 #print(predicted, measured) mean_squared_error = np . mean ( squared_error ) variance_error = np . var ( squared_error ) std_error = np . std ( squared_error ) # find max and min inside bounds using scipy.optimize from scipy.optimize import minimize mx = minimize ( lambda x : - 1 * regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) max_point = mx . x max_fun = - 1 * mx . fun mn = minimize ( lambda x : regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) min_point = mn . x min_fun = mn . fun #mse = mean_squared_error(predicted, measured) # mean squared error #mse = np.sum((predicted - measured)**2)/len(predicted) #alternative calculation print ( \"Mean squared error :\" , mean_squared_error ) print ( \"Variance of error :\" , variance_error ) print ( \"Standard dev. error:\" , std_error ) print ( \"Fitting score. :\" , score ) print ( \"Maximum coords :\" , max_point ) print ( \"Maximum value. :\" , max_fun [ 0 ]) print ( \"Minimum coords :\" , min_point ) print ( \"Minimum value. :\" , min_fun [ 0 ]) xa = np . linspace ( bounds [ 0 , 0 ], bounds [ 0 , 1 ], Nx ) xb = np . linspace ( bounds [ 1 , 0 ], bounds [ 1 , 1 ], Nx ) xc = np . linspace ( bounds [ 2 , 0 ], bounds [ 2 , 1 ], Nx ) va , vb , vc = bbwidget . column_headers [ 1 ], bbwidget . column_headers [ 2 ], bbwidget . column_headers [ 3 ] # displaying the fitting parameters fnames = relabel_defaults ( poly . get_feature_names (), [ va , vb , vc ]) ax , fig = plt . subplots ( figsize = ( 9 , 5 )) plt . plot ( regressor . steps [ 1 ][ 1 ] . coef_ , \"s\" ) #fig.set_xticklabels(poly.get_feature_names()) for i in range ( len ( regressor . steps [ 1 ][ 1 ] . coef_ )): plt . text ( i + .1 , regressor . steps [ 1 ][ 1 ] . coef_ [ i ], fnames [ i ], ha = \"left\" , va = \"center\" ) plt . axhline ( 0 ) plt . title ( \"Fitting parameters\" ) plt . show () # print a table of the fitting parameters print ( np . array ([ fnames , regressor . steps [ 1 ][ 1 ] . coef_ ]) . T ) print ( \"Intercept:\" , regressor . steps [ 1 ][ 1 ] . intercept_ ) \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vb) plt.contourf(xa,xb,yab) plt.xlabel(va) plt.ylabel(vb) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vc) plt.contourf(xa,xc,yac) plt.xlabel(va) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(vb + \" vs \" + vc) plt.contourf(xb,xc,ybc) plt.xlabel(vb) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" #fig = plt.figure(figsize=(9,3)) fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 ), sharey = True ) #fig. #ax = fig.add_subplot(1, 3, 1) Xa = np . zeros (( Nx , 3 ), dtype = float ) Xa [:, 0 ] = xa Xa [:, 1 :] = min_point [ 1 :] ax1 . plot ( xa , regressor . predict ( Xa )) ax1 . set_xlabel ( va ) #ax = fig.add_subplot(1, 3, 2) Xb = np . zeros (( Nx , 3 ), dtype = float ) Xb [:, 1 ] = xb Xb [:, 0 ] = min_point [ 0 ] Xb [:, 2 ] = min_point [ 2 ] ax2 . plot ( xb , regressor . predict ( Xb )) ax2 . set_xlabel ( vb ) ax2 . set_title ( \"Fitted means\" ) #ax = fig.add_subplot(1, 3, 3) Xc = np . zeros (( Nx , 3 ), dtype = float ) Xc [:, 2 ] = xc Xc [:, 0 ] = min_point [ 0 ] Xc [:, 1 ] = min_point [ 1 ] ax3 . plot ( xc , regressor . predict ( Xc )) ax3 . set_xlabel ( vc ) #ax.show() #plt.show() xab3 = np . vstack (( np . array ( np . meshgrid ( xa , xb )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T yab = regressor . predict ( xab3 ) . reshape (( Nx , Nx )) #fig, ax = plt.subplots() #subplot_kw={\"projection\": \"3d\"}) fig = plt . figure ( figsize = ( 9 , 3 )) ax = fig . add_subplot ( 1 , 3 , 1 , projection = '3d' ) X , Y = np . meshgrid ( xa , xb ) surf = ax . plot_surface ( X , Y , yab , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yab , zdir = 'z' , offset = yab . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vb ) #plt.show() xac3 = np . vstack (( np . array ( np . meshgrid ( xa , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xac3 [:,[ 1 , 2 ]] = xac3 [:, [ 2 , 1 ]] yac = regressor . predict ( xac3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 2 , projection = '3d' ) X , Y = np . meshgrid ( xa , xc ) surf = ax . plot_surface ( X , Y , yac , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yac , zdir = 'z' , offset = yac . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vc ) #plt.show() xbc3 = np . vstack (( np . array ( np . meshgrid ( xb , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xbc3 [:,[ 0 , 1 , 2 ]] = xac3 [:, [ 1 , 0 , 2 ]] ybc = regressor . predict ( xbc3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 3 , projection = '3d' ) X , Y = np . meshgrid ( xb , xc ) surf = ax . plot_surface ( X , Y , ybc , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , ybc , zdir = 'z' , offset = ybc . min (), cmap = cm . coolwarm ) plt . xlabel ( vb ) plt . ylabel ( vc ) plt . show ()","title":"bbtools"},{"location":"bbtools-reference/#box-behnken-tools-bbtools","text":"","title":"Box-Behnken Tools (bbtools)"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget","text":"Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022 Source code in btjenesten/bbtools.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class tablewidget (): \"\"\" Tabular in/out for Box-Behnken widgets Author: Audun Skau Hansen, 2022 \"\"\" def __init__ ( self , column_headers , row_headers ): self . tab = np . zeros (( len ( row_headers ) + 1 , len ( column_headers ) + 1 ), dtype = object ) #self.tab[1:,0] = row_headers #self.tab[0,1:] = column_headers self . row_headers = row_headers self . column_headers = column_headers self . tab [ 0 , 0 ] = widgets . Label ( \"\" ) for i in range ( len ( column_headers )): self . tab [ 0 , i + 1 ] = widgets . Label ( column_headers [ i ], align = \"right\" ) for j in range ( len ( row_headers )): self . tab [ j + 1 , 0 ] = widgets . Label ( row_headers [ j ], align = \"center\" ) self . items = [] for i in range ( len ( row_headers )): for j in range ( len ( column_headers )): self . tab [ i + 1 , j + 1 ] = widgets . BoundedFloatText ( value = 0 , min =- 1e15 , max = 1e15 , step = 0.1 , description = '' , disabled = False ) self . widget = widgets . GridBox ( list ( self . tab . ravel ()), layout = widgets . Layout ( grid_template_columns = \"repeat( %i , 100px)\" % ( len ( column_headers ) + 1 ))) def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ]) def _repr ( self ): \"\"\" Returns a latex-formatted string to display the mathematical expression of the basisfunction. \"\"\" return self . widget","title":"tablewidget"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget.as_numpy_array","text":"Returns the table (excluding headers) as a numpy array Source code in btjenesten/bbtools.py 47 48 49 50 51 52 53 54 55 def as_numpy_array ( self ): \"\"\" Returns the table (excluding headers) as a numpy array \"\"\" ret = np . zeros ( self . tab [ 1 :, 1 :] . shape , dtype = float ) for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): ret [ i , j ] = float ( self . tab [ i + 1 , j + 1 ] . value ) return ret","title":"as_numpy_array()"},{"location":"bbtools-reference/#btjenesten.bbtools.tablewidget.set_from_array","text":"Set the table (excluding headers) from an input array Source code in btjenesten/bbtools.py 57 58 59 60 61 62 63 def set_from_array ( self , input_array ): \"\"\" Set the table (excluding headers) from an input array \"\"\" for i in range ( self . tab . shape [ 0 ] - 1 ): for j in range ( self . tab . shape [ 1 ] - 1 ): self . tab [ i + 1 , j + 1 ] . value = str ( input_array [ i , j ])","title":"set_from_array()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbdesign","text":"Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"bbdesign()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbdesign--keyword-arguments","text":"Argument Description n_center number of samples in the center randomize whether or not to randomize the ordering (bool) sheet heet containing the min/max values of variables Source code in btjenesten/bbtools.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def bbdesign ( n_center = 3 , randomize = True , sheet = None ): \"\"\" Returns a Box-Benhken experimental design for 3 variables Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | n_center | number of samples in the center | | randomize | whether or not to randomize the ordering (bool) | | sheet | heet containing the min/max values of variables | \"\"\" a = np . arange ( - 1 , 2 ) A = np . array ( np . meshgrid ( a , a , a )) . reshape ( 3 , - 1 ) . T A = np . concatenate ([ A [ np . sum ( A ** 2 , axis = 1 ) == 2 , :], np . zeros (( n_center , 3 ))]) ai = np . arange ( len ( A )) if randomize == True : # randomize run order np . random . shuffle ( ai ) if sheet is not None : # Transform coordinates tm = sheet . as_numpy_array () for i in range ( 3 ): A [:, i ] = interp1d ( np . linspace ( - 1 , 1 , 2 ), tm [ i ] )( A [:, i ]) #A = A.dot(tm) return A [ ai , :], ai","title":"Keyword arguments:"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsetup","text":"Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 def bbsetup (): \"\"\" Returns an interactive sheet (ipysheet) for setting up a Box-Benkhen design session **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" global sheet arr = np . zeros (( 4 , 3 ), dtype = object ) arr [ 0 , 0 ] = \"\" arr [ 1 , 0 ] = \"Variable A\" arr [ 2 , 0 ] = \"Variable B\" arr [ 3 , 0 ] = \"Variable C\" arr [ 0 , 1 ] = \"Minimum\" arr [ 0 , 2 ] = \"Maximum\" arr [ 1 :, 1 ] = - 1 arr [ 1 :, 2 ] = 1 sheet = from_array ( arr ) sheet . column_headers = False sheet . row_headers = False return sheet","title":"bbsetup()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsheet","text":"Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"bbsheet()"},{"location":"bbtools-reference/#btjenesten.bbtools.bbsheet--keyword-arguments","text":"sheet = setup from bbsetup Source code in btjenesten/bbtools.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def bbsheet ( sheet ): \"\"\" Returns a Box-Behnken sheet for gathering experimental results Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments sheet = setup from bbsetup \"\"\" bd , ai = bbdesign ( sheet = sheet ) #global bb_sheet #sh = sheet.as_numpy_array() #arr = np.zeros(sh.shape + np.array([1,2]), dtype = object) #print(\"sh\") #print(sh) #print(bd) column_headers = [ \"Run\" , sheet . row_headers [ 0 ], sheet . row_headers [ 1 ], sheet . row_headers [ 2 ], \"Result\" ] row_headers = [ \"\" for i in range ( bd . shape [ 0 ])] bb_widget = tablewidget ( column_headers , row_headers ) arr = np . zeros ( ( len ( row_headers ), len ( column_headers )), dtype = float ) arr [:, 0 ] = np . arange ( len (( row_headers ))) + 1 arr [:, 0 ] = ai + 1 arr [:, 1 : 4 ] = bd bb_widget . set_from_array ( arr ) return bb_widget","title":"Keyword arguments"},{"location":"bbtools-reference/#btjenesten.bbtools.minitable","text":"Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO","title":"minitable()"},{"location":"bbtools-reference/#btjenesten.bbtools.minitable--keyword-arguments","text":"Argument Description titles default variables values coefficients sheet the Box-Behnken sheet Source code in btjenesten/bbtools.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def minitable ( titles , values , sheet ): \"\"\" Generate a mini-table for displaying inter-variable dependencies as indicated by the model Author: Audun Skau Hansen, Department of Chemistry, UiO ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | titles | default variables | | values | coefficients | | sheet | the Box-Behnken sheet | \"\"\" arr = np . zeros (( len ( titles ), 2 ), dtype = object ) arr [:, 0 ] = relabel_defaults ( titles , sheet ) arr [:, 1 ] = values return from_array ( arr )","title":"Keyword arguments:"},{"location":"bbtools-reference/#btjenesten.bbtools.relabel_defaults","text":"Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] Author : Audun Skau Hansen, Department of Chemistry, UiO Source code in btjenesten/bbtools.py 311 312 313 314 315 316 317 318 319 320 321 def relabel_defaults ( titles , new_names ): \"\"\" Rename default variable (\"x0\", \"x1\", \"x2\") to variable names from sheet[1:,0] **Author**: Audun Skau Hansen, Department of Chemistry, UiO \"\"\" #new_names = to_array(sheet)[1:4,0] new_titles = [] for i in titles : new_titles . append ( i . replace ( \"x0\" , new_names [ 0 ]) . replace ( \"x1\" , new_names [ 1 ]) . replace ( \"x2\" , new_names [ 2 ]) ) return new_titles","title":"relabel_defaults()"},{"location":"bbtools-reference/#btjenesten.bbtools.visualize_surfaces","text":"Visualize response surfaces the regressor model Author : Audun Skau Hansen, Department of Chemistry, UiO (2022)","title":"visualize_surfaces()"},{"location":"bbtools-reference/#btjenesten.bbtools.visualize_surfaces--keyword-arguments","text":"Argument Description sheet Box-Benhken data sheet regressor sklearn LinearRegression instance Nx mesh resolution along each axis Source code in btjenesten/bbtools.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def visualize_surfaces ( bbwidget , Nx = 30 ): \"\"\" Visualize response surfaces the regressor model **Author**: Audun Skau Hansen, Department of Chemistry, UiO (2022) ## Keyword arguments: | Argument | Description | | ----------- | ----------- | | sheet | Box-Benhken data sheet | | regressor | sklearn LinearRegression instance | | Nx | mesh resolution along each axis | \"\"\" data = bbwidget . as_numpy_array () bounds = np . zeros (( 3 , 2 ), dtype = float ) bounds [:, 0 ] = np . min ( data [:, 1 : 4 ], axis = 0 ) bounds [:, 1 ] = np . max ( data [:, 1 : 4 ], axis = 0 ) #if regressor is None: # crop data from the sheet above X_train = data [:, 1 : 4 ] y_train = data [:, 4 ] #print(X_train, y_train) # perform a second order polynomial fit # (linear in the matrix elements) degree = 2 # second order poly = PolynomialFeatures ( degree ) # these are the matrix elements regressor = make_pipeline ( poly , LinearRegression ()) #set up the regressor regressor . fit ( X_train , y_train ) # fit the model # first, we extract all relevant information coefficients = regressor . steps [ 1 ][ 1 ] . coef_ names = poly . get_feature_names () predicted = regressor . predict ( X_train ) measured = y_train score = regressor . score ( X_train , y_train ) # we then compute and tabulate various statistics squared_error = ( predicted - measured ) ** 2 #print(predicted, measured) mean_squared_error = np . mean ( squared_error ) variance_error = np . var ( squared_error ) std_error = np . std ( squared_error ) # find max and min inside bounds using scipy.optimize from scipy.optimize import minimize mx = minimize ( lambda x : - 1 * regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) max_point = mx . x max_fun = - 1 * mx . fun mn = minimize ( lambda x : regressor . predict ( np . array ([ x ])), X_train [ 0 ], bounds = bounds ) min_point = mn . x min_fun = mn . fun #mse = mean_squared_error(predicted, measured) # mean squared error #mse = np.sum((predicted - measured)**2)/len(predicted) #alternative calculation print ( \"Mean squared error :\" , mean_squared_error ) print ( \"Variance of error :\" , variance_error ) print ( \"Standard dev. error:\" , std_error ) print ( \"Fitting score. :\" , score ) print ( \"Maximum coords :\" , max_point ) print ( \"Maximum value. :\" , max_fun [ 0 ]) print ( \"Minimum coords :\" , min_point ) print ( \"Minimum value. :\" , min_fun [ 0 ]) xa = np . linspace ( bounds [ 0 , 0 ], bounds [ 0 , 1 ], Nx ) xb = np . linspace ( bounds [ 1 , 0 ], bounds [ 1 , 1 ], Nx ) xc = np . linspace ( bounds [ 2 , 0 ], bounds [ 2 , 1 ], Nx ) va , vb , vc = bbwidget . column_headers [ 1 ], bbwidget . column_headers [ 2 ], bbwidget . column_headers [ 3 ] # displaying the fitting parameters fnames = relabel_defaults ( poly . get_feature_names (), [ va , vb , vc ]) ax , fig = plt . subplots ( figsize = ( 9 , 5 )) plt . plot ( regressor . steps [ 1 ][ 1 ] . coef_ , \"s\" ) #fig.set_xticklabels(poly.get_feature_names()) for i in range ( len ( regressor . steps [ 1 ][ 1 ] . coef_ )): plt . text ( i + .1 , regressor . steps [ 1 ][ 1 ] . coef_ [ i ], fnames [ i ], ha = \"left\" , va = \"center\" ) plt . axhline ( 0 ) plt . title ( \"Fitting parameters\" ) plt . show () # print a table of the fitting parameters print ( np . array ([ fnames , regressor . steps [ 1 ][ 1 ] . coef_ ]) . T ) print ( \"Intercept:\" , regressor . steps [ 1 ][ 1 ] . intercept_ ) \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vb) plt.contourf(xa,xb,yab) plt.xlabel(va) plt.ylabel(vb) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(va + \" vs \" + vc) plt.contourf(xa,xc,yac) plt.xlabel(va) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" \"\"\" plt.figure(figsize=(9.5,8)) plt.title(vb + \" vs \" + vc) plt.contourf(xb,xc,ybc) plt.xlabel(vb) plt.ylabel(vc) plt.colorbar() plt.show() \"\"\" #fig = plt.figure(figsize=(9,3)) fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 9 , 4 ), sharey = True ) #fig. #ax = fig.add_subplot(1, 3, 1) Xa = np . zeros (( Nx , 3 ), dtype = float ) Xa [:, 0 ] = xa Xa [:, 1 :] = min_point [ 1 :] ax1 . plot ( xa , regressor . predict ( Xa )) ax1 . set_xlabel ( va ) #ax = fig.add_subplot(1, 3, 2) Xb = np . zeros (( Nx , 3 ), dtype = float ) Xb [:, 1 ] = xb Xb [:, 0 ] = min_point [ 0 ] Xb [:, 2 ] = min_point [ 2 ] ax2 . plot ( xb , regressor . predict ( Xb )) ax2 . set_xlabel ( vb ) ax2 . set_title ( \"Fitted means\" ) #ax = fig.add_subplot(1, 3, 3) Xc = np . zeros (( Nx , 3 ), dtype = float ) Xc [:, 2 ] = xc Xc [:, 0 ] = min_point [ 0 ] Xc [:, 1 ] = min_point [ 1 ] ax3 . plot ( xc , regressor . predict ( Xc )) ax3 . set_xlabel ( vc ) #ax.show() #plt.show() xab3 = np . vstack (( np . array ( np . meshgrid ( xa , xb )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T yab = regressor . predict ( xab3 ) . reshape (( Nx , Nx )) #fig, ax = plt.subplots() #subplot_kw={\"projection\": \"3d\"}) fig = plt . figure ( figsize = ( 9 , 3 )) ax = fig . add_subplot ( 1 , 3 , 1 , projection = '3d' ) X , Y = np . meshgrid ( xa , xb ) surf = ax . plot_surface ( X , Y , yab , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yab , zdir = 'z' , offset = yab . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vb ) #plt.show() xac3 = np . vstack (( np . array ( np . meshgrid ( xa , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xac3 [:,[ 1 , 2 ]] = xac3 [:, [ 2 , 1 ]] yac = regressor . predict ( xac3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 2 , projection = '3d' ) X , Y = np . meshgrid ( xa , xc ) surf = ax . plot_surface ( X , Y , yac , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , yac , zdir = 'z' , offset = yac . min (), cmap = cm . coolwarm ) plt . xlabel ( va ) plt . ylabel ( vc ) #plt.show() xbc3 = np . vstack (( np . array ( np . meshgrid ( xb , xc )) . reshape ( 2 , - 1 ), np . zeros ( Nx ** 2 ))) . T xbc3 [:,[ 0 , 1 , 2 ]] = xac3 [:, [ 1 , 0 , 2 ]] ybc = regressor . predict ( xbc3 ) . reshape (( Nx , Nx )) ax = fig . add_subplot ( 1 , 3 , 3 , projection = '3d' ) X , Y = np . meshgrid ( xb , xc ) surf = ax . plot_surface ( X , Y , ybc , linewidth = 0 , antialiased = False , cmap = cm . coolwarm ) #fig.colorbar(surf, shrink=0.3, aspect=5) ax . contour ( X , Y , ybc , zdir = 'z' , offset = ybc . min (), cmap = cm . coolwarm ) plt . xlabel ( vb ) plt . ylabel ( vc ) plt . show ()","title":"Keyword arguments:"},{"location":"btjenesten-reference/","text":"B-tjenesten","title":"btjenesten"},{"location":"btjenesten-reference/#b-tjenesten","text":"","title":"B-tjenesten"},{"location":"gpr-reference/","text":"gpr Author : Christian Elias Anderssen Dalan ceadyy@gmail.com , With the help of Audun Skau Hansen a.s.hansen@kjemi.uio.no April 2022 Kernel Kernel class Author: Christian Elias Anderssen Dalan ceadyy@gmail.com covariance_function: The function that will be used to calculate the covariance between our datasets Source code in btjenesten/gpr.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Kernel (): \"\"\" Kernel class Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- covariance_function: The function that will be used to calculate the covariance between our datasets \"\"\" def __init__ ( self , covariance_function ): self . covariance_function = covariance_function def K ( self , X1 , X2 , params ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 , params ) K ( X1 , X2 , params ) Function that returns the covariance matrix given our datasets X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. Source code in btjenesten/gpr.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def K ( self , X1 , X2 , params ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 , params ) Regressor Gaussian process regressor class Author: Christian Elias Anderssen Dalan ceadyy@gmail.com kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels Source code in btjenesten/gpr.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 class Regressor (): \"\"\" Gaussian process regressor class Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels params: \"\"\" def __init__ ( self , training_data_X , training_data_Y , kernel = None , params = 1 , normalize = False , normalize_log = False ): if kernel == None : self . kernel = Kernel ( RBF ) else : self . kernel = Kernel ( kernel ) msg = \"Expected 2D array. If you only have one feature reshape training data using array.reshape(-1, 1)\" assert training_data_X . ndim != 1 , msg if normalize : \"\"\" Normalize X and Y data \"\"\" self . normalize , self . recover = normalize_training_data_x ( training_data_X ) self . training_data_X = self . normalize ( training_data_X ) self . normalization_factor_y = np . abs ( training_data_Y ) . max () self . training_data_Y = training_data_Y / self . normalization_factor_y else : self . normalize , self . recover = no_normalization ( training_data_X ) self . normalization_factor_y = 1.0 self . training_data_X = training_data_X self . training_data_Y = training_data_Y if normalize_log : \"\"\" Normalize X and Y data \"\"\" self . normalize , self . recover = normalize_training_data_x_log ( training_data_X ) self . training_data_X = self . normalize ( training_data_X ) self . normalization_factor_y = np . abs ( training_data_Y ) . max () self . training_data_Y = training_data_Y / self . normalization_factor_y self . params = 1 # def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. \"\"\" if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X , self . params ) K_12 = self . kernel . K ( self . training_data_X , self . normalize ( input_data_X ), self . params ) K_21 = K_12 . T K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( training_data_X ), self . params ) K_12 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( input_data_X ), self . params ) K_21 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( training_data_X ), self . params ) K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () * self . normalization_factor_y if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error , max_error def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 , method = \"COBYLA\" ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yield the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p def update ( self , new_X , new_Y , tol = 1e-5 ): \"\"\" Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" assert type ( new_Y ) is np . ndarray , \"Data error!!!!! Needs to be array.\" assert type ( new_X ) is np . ndarray , \"Data error!!!!! Needs to be array.\" for measurement in new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]): for i in range ( len ( self . training_data_X )): if np . allclose ( measurement , self . training_data_X [ i ], atol = tol ): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return True \"\"\" old_X_shape = self.training_data_X.shape old_Y_shape = len(self.training_data_Y) new_X_shape = np.array(self.training_data_X.shape) new_Y_shape = len(new_Y) new_X_shape[0] += new_X.shape[0] new_Y_shape += len(new_Y) new_training_data_X = np.zeros(new_X_shape) new_training_data_Y = np.zeros(new_Y_shape) new_training_data_X[:-old_X_shape.shape[0]] = self.training_data_X new_training_data_X[-new_X.shape[0]:] = new_X new_training_data_Y[:-old_Y_shape] = self.training_data_Y new_training_data_Y[-new_Y_shape:] = new_Y \"\"\" #print(\"X1 shape \",self.training_data_X.shape) #print(\"X2 shape \",.shape) new_X = new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]) new_training_data_X = np . concatenate (( self . training_data_X , self . normalize ( new_X ))) new_training_data_Y = np . concatenate (( self . training_data_Y , new_Y / self . normalization_factor_y )) #indexes = np.argsort(new_training_data_X) self . training_data_X = new_training_data_X #[indexes] self . training_data_Y = new_training_data_Y #[indexes] return False aquisition ( minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 , method = 'COBYLA' ) Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. p - The predicted point at which an evaluation would yield the highest/lowest value Source code in btjenesten/gpr.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 , method = \"COBYLA\" ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yield the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p predict ( input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ) Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan ceadyy@gmail.com input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. Source code in btjenesten/gpr.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. \"\"\" if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X , self . params ) K_12 = self . kernel . K ( self . training_data_X , self . normalize ( input_data_X ), self . params ) K_21 = K_12 . T K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( training_data_X ), self . params ) K_12 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( input_data_X ), self . params ) K_21 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( training_data_X ), self . params ) K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () * self . normalization_factor_y if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y score ( input_data_X , input_data_Y ) Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values Source code in btjenesten/gpr.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error , max_error update ( new_X , new_Y , tol = 1e-05 ) Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. Source code in btjenesten/gpr.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update ( self , new_X , new_Y , tol = 1e-5 ): \"\"\" Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" assert type ( new_Y ) is np . ndarray , \"Data error!!!!! Needs to be array.\" assert type ( new_X ) is np . ndarray , \"Data error!!!!! Needs to be array.\" for measurement in new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]): for i in range ( len ( self . training_data_X )): if np . allclose ( measurement , self . training_data_X [ i ], atol = tol ): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return True \"\"\" old_X_shape = self.training_data_X.shape old_Y_shape = len(self.training_data_Y) new_X_shape = np.array(self.training_data_X.shape) new_Y_shape = len(new_Y) new_X_shape[0] += new_X.shape[0] new_Y_shape += len(new_Y) new_training_data_X = np.zeros(new_X_shape) new_training_data_Y = np.zeros(new_Y_shape) new_training_data_X[:-old_X_shape.shape[0]] = self.training_data_X new_training_data_X[-new_X.shape[0]:] = new_X new_training_data_Y[:-old_Y_shape] = self.training_data_Y new_training_data_Y[-new_Y_shape:] = new_Y \"\"\" #print(\"X1 shape \",self.training_data_X.shape) #print(\"X2 shape \",.shape) new_X = new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]) new_training_data_X = np . concatenate (( self . training_data_X , self . normalize ( new_X ))) new_training_data_Y = np . concatenate (( self . training_data_Y , new_Y / self . normalization_factor_y )) #indexes = np.argsort(new_training_data_X) self . training_data_X = new_training_data_X #[indexes] self . training_data_Y = new_training_data_Y #[indexes] return False no_normalization ( training_data ) generate functions which essentially does nothing Author: No-one Source code in btjenesten/gpr.py 374 375 376 377 378 379 380 381 382 383 384 385 386 def no_normalization ( training_data ): \"\"\" generate functions which essentially does nothing Author: No-one \"\"\" def normalize ( training_data ): return training_data def recover ( training_data ): return training_data return normalize , recover normalize_training_data_x ( training_data ) generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) Source code in btjenesten/gpr.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def normalize_training_data_x ( training_data ): \"\"\" generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) \"\"\" mean = np . mean ( training_data , axis = 0 ) bound = np . max ( training_data , axis = 0 ) - np . min ( training_data , axis = 0 ) def normalize ( training_data , mean = mean , bound = bound ): training_data_normalized = training_data - mean [ None ,:] training_data_normalized *= ( .5 * bound [ None , :]) **- 1 return training_data_normalized def recover ( training_data_normalized , mean = mean , bound = bound ): return training_data_normalized * ( .5 * bound [ None ,:]) + mean [ None , :] return normalize , recover normalize_training_data_x_log ( training_data ) generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) Source code in btjenesten/gpr.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def normalize_training_data_x_log ( training_data ): \"\"\" generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) \"\"\" #mean = np.mean(training_data, axis =0 ) min_ = np . min ( training_data , axis = 0 ) - 1e-3 bound = np . max ( training_data , axis = 0 ) - np . min ( training_data , axis = 0 ) def normalize ( training_data , min_ = min_ , bound = bound ): training_data_normalized = training_data return np . log ( training_data_normalized ) def recover ( training_data_normalized , min_ = min_ , bound = bound ): return np . exp ( training_data_normalized ) return normalize , recover Constant ( X1 , X2 , k = 0.5 ) Kernel that returns a constant covariance value between \\(x_i\\) and \\(x_j\\) Useful if all values depend on eachother equally. X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. A matrix with the same shape as our input data. All matrix elements have the value k. Source code in btjenesten/kernels.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def Constant ( X1 , X2 , k = 0.5 ): \"\"\" Kernel that returns a constant covariance value between $x_i$ and $x_j$ Useful if all values depend on eachother equally. Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. Returns: A matrix with the same shape as our input data. All matrix elements have the value k. \"\"\" return np . ones ( X1 . shape ) * k Funny_trigonometric ( X1 , X2 , k = 1 ) Kernel that I made only for fun. May work for extravagant datasets X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. A covariance matrix that might be a bit crazy. Source code in btjenesten/kernels.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def Funny_trigonometric ( X1 , X2 , k = 1 ): \"\"\" Kernel that I made only for fun. May work for extravagant datasets Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. Returns: A covariance matrix that might be a bit crazy. \"\"\" return np . sin ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) - np . cos ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) RBF ( X1 , X2 , l = np . array ([ 1.0 ])) Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between \\(x_i\\) and \\(x_j\\) . Increasing l will decrease the covariance, and vice versa. A matrix with the same shape as our input data, where the elemets are: \\(e^{-l \\cdot d(x_i, x_j)}\\) where \\(d(x_i, x_j)\\) is the difference between element \\(x_i\\) in X1 and element \\(x_j\\) in X2. Source code in btjenesten/kernels.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def RBF ( X1 , X2 , l = np . array ([ 1.0 ])): \"\"\" Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ Parameters: ----------- X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between $x_i$ and $x_j$. Increasing l will decrease the covariance, and vice versa. Returns: ----------- A matrix with the same shape as our input data, where the elemets are: $e^{-l \\cdot d(x_i, x_j)}$ where $d(x_i, x_j)$ is the difference between element $x_i$ in X1 and element $x_j$ in X2. \"\"\" if type ( l ) is not np . ndarray : # patch for scalar length parameter l = np . array ([ l ]) ld = np . sum ( l [ None , :] * ( X1 . reshape ( X1 . shape [ 0 ], - 1 )[:, None ] - X2 . reshape ( X2 . shape [ 0 ], - 1 )[ None ,]) ** 2 , axis = 2 ) return np . exp ( - ld )","title":"gpr"},{"location":"gpr-reference/#gpr","text":"Author : Christian Elias Anderssen Dalan ceadyy@gmail.com , With the help of Audun Skau Hansen a.s.hansen@kjemi.uio.no April 2022","title":"gpr"},{"location":"gpr-reference/#btjenesten.gpr.Kernel","text":"Kernel class Author: Christian Elias Anderssen Dalan ceadyy@gmail.com covariance_function: The function that will be used to calculate the covariance between our datasets Source code in btjenesten/gpr.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Kernel (): \"\"\" Kernel class Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- covariance_function: The function that will be used to calculate the covariance between our datasets \"\"\" def __init__ ( self , covariance_function ): self . covariance_function = covariance_function def K ( self , X1 , X2 , params ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 , params )","title":"Kernel"},{"location":"gpr-reference/#btjenesten.gpr.Kernel.K","text":"Function that returns the covariance matrix given our datasets X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. Source code in btjenesten/gpr.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def K ( self , X1 , X2 , params ): \"\"\" Function that returns the covariance matrix given our datasets Parameters: ----------- X1: Dataset 1 (Often the training set) X2: Dataset 2 (Often the target set) Returns: ---------- self.covariance_function(X1, X2) : covariance matrix given our datasets X1 and X2. \"\"\" if np . isscalar ( X1 ): X1 = np . array ([ X1 ]) if np . isscalar ( X2 ): X2 = np . array ([ X2 ]) return self . covariance_function ( X1 , X2 , params )","title":"K()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor","text":"Gaussian process regressor class Author: Christian Elias Anderssen Dalan ceadyy@gmail.com kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels Source code in btjenesten/gpr.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 class Regressor (): \"\"\" Gaussian process regressor class Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- kernel: Specifies the type of covarince function we want for our regressor. If none is provided the default is the radial basis function training_data_X: Training data inputs, also called features training_data_Y: Training data outputs, also called labels params: \"\"\" def __init__ ( self , training_data_X , training_data_Y , kernel = None , params = 1 , normalize = False , normalize_log = False ): if kernel == None : self . kernel = Kernel ( RBF ) else : self . kernel = Kernel ( kernel ) msg = \"Expected 2D array. If you only have one feature reshape training data using array.reshape(-1, 1)\" assert training_data_X . ndim != 1 , msg if normalize : \"\"\" Normalize X and Y data \"\"\" self . normalize , self . recover = normalize_training_data_x ( training_data_X ) self . training_data_X = self . normalize ( training_data_X ) self . normalization_factor_y = np . abs ( training_data_Y ) . max () self . training_data_Y = training_data_Y / self . normalization_factor_y else : self . normalize , self . recover = no_normalization ( training_data_X ) self . normalization_factor_y = 1.0 self . training_data_X = training_data_X self . training_data_Y = training_data_Y if normalize_log : \"\"\" Normalize X and Y data \"\"\" self . normalize , self . recover = normalize_training_data_x_log ( training_data_X ) self . training_data_X = self . normalize ( training_data_X ) self . normalization_factor_y = np . abs ( training_data_Y ) . max () self . training_data_Y = training_data_Y / self . normalization_factor_y self . params = 1 # def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. \"\"\" if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X , self . params ) K_12 = self . kernel . K ( self . training_data_X , self . normalize ( input_data_X ), self . params ) K_21 = K_12 . T K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( training_data_X ), self . params ) K_12 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( input_data_X ), self . params ) K_21 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( training_data_X ), self . params ) K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () * self . normalization_factor_y if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error , max_error def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 , method = \"COBYLA\" ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yield the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p def update ( self , new_X , new_Y , tol = 1e-5 ): \"\"\" Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" assert type ( new_Y ) is np . ndarray , \"Data error!!!!! Needs to be array.\" assert type ( new_X ) is np . ndarray , \"Data error!!!!! Needs to be array.\" for measurement in new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]): for i in range ( len ( self . training_data_X )): if np . allclose ( measurement , self . training_data_X [ i ], atol = tol ): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return True \"\"\" old_X_shape = self.training_data_X.shape old_Y_shape = len(self.training_data_Y) new_X_shape = np.array(self.training_data_X.shape) new_Y_shape = len(new_Y) new_X_shape[0] += new_X.shape[0] new_Y_shape += len(new_Y) new_training_data_X = np.zeros(new_X_shape) new_training_data_Y = np.zeros(new_Y_shape) new_training_data_X[:-old_X_shape.shape[0]] = self.training_data_X new_training_data_X[-new_X.shape[0]:] = new_X new_training_data_Y[:-old_Y_shape] = self.training_data_Y new_training_data_Y[-new_Y_shape:] = new_Y \"\"\" #print(\"X1 shape \",self.training_data_X.shape) #print(\"X2 shape \",.shape) new_X = new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]) new_training_data_X = np . concatenate (( self . training_data_X , self . normalize ( new_X ))) new_training_data_Y = np . concatenate (( self . training_data_Y , new_Y / self . normalization_factor_y )) #indexes = np.argsort(new_training_data_X) self . training_data_X = new_training_data_X #[indexes] self . training_data_Y = new_training_data_Y #[indexes] return False","title":"Regressor"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.aquisition","text":"Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. p - The predicted point at which an evaluation would yield the highest/lowest value Source code in btjenesten/gpr.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def aquisition ( self , minimize_prediction = True , x0 = None , l = 1.2 , delta = 0.1 , method = \"COBYLA\" ): \"\"\" Returns the point at which our model function is predicted to have the highest value. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- minimize_prediction: If your task is to minimize some model function, this parameter is True. If your task is to maximize the model function this parameter is False. l: Exploration parameter. Scales how much the standard deviation should impact the function value. l = 1 means that the function maximized/minimized equals predicted value +/- the standard deviation. x0: Initial guess. If not specified it will use the point at which the training data is the largest/smallest. delta: Hyperparameter that tunes UCB around measured datapoints. Returns: -------- p - The predicted point at which an evaluation would yield the highest/lowest value \"\"\" if minimize_prediction : #Minimization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . min ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : exploit ( x ) + l * explore ( x ) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p else : #Maximization process if x0 == None : x0_index = np . where ( self . training_data_Y == np . max ( self . training_data_Y )) x0 = self . training_data_X [ x0_index ] objective_function = lambda x , predict = self . predict : predict ( x ) std_x = lambda x , predict = self . predict : np . sqrt ( np . abs ( np . diag ( predict ( x , return_variance = True )[ 1 ]))) objective_noise = lambda x , std = std_x : ( 1 - std ( x )) ** 2 * delta + std ( x ) UCB = lambda x , exploit = objective_function , explore = objective_noise : - 1 * ( exploit ( x ) + l * explore ( x )) def UCB ( x , f = UCB ): x = x . reshape ( 1 , - 1 ) return f ( x ) minimization = minimize ( UCB , x0 , method = method ) p = minimization . x return p","title":"aquisition()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.predict","text":"Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan ceadyy@gmail.com input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. Source code in btjenesten/gpr.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def predict ( self , input_data_X , training_data_X = None , training_data_Y = None , return_variance = False ): \"\"\" Predicts output values for some input data given a set of training data Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: Input features that the gpr will evaluate. training_data_X: training data inputs. training_data_Y: training data outputs. return_variance: Returns variance for each prediction if this is true Returns: ----------- predicted_y: Predicted output data given cooresponding input_data_X and a set of training data inputs and outputs (training_data_X, training_data_Y) predicted_variance: Predicted variance for each point of predicted output. \"\"\" if training_data_X == None or training_data_Y == None : K_11 = self . kernel . K ( self . training_data_X , self . training_data_X , self . params ) K_12 = self . kernel . K ( self . training_data_X , self . normalize ( input_data_X ), self . params ) K_21 = K_12 . T K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( self . training_data_Y ) else : K_11 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( training_data_X ), self . params ) K_12 = self . kernel . K ( self . normalize ( training_data_X ), self . normalize ( input_data_X ), self . params ) K_21 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( training_data_X ), self . params ) K_22 = self . kernel . K ( self . normalize ( input_data_X ), self . normalize ( input_data_X ), self . params ) assert ( np . linalg . det ( K_11 ) != 0 ), \"Singular matrix. Training data might have duplicates.\" KT = np . linalg . solve ( K_11 , K_12 ) . T predicted_y = KT . dot ( training_data_Y ) predicted_y = predicted_y . ravel () * self . normalization_factor_y if return_variance : predicted_variance = np . diag ( K_22 - KT @ K_12 ) y_var_negative = predicted_variance < 0 if np . any ( y_var_negative ): predicted_variance . setflags ( write = \"True\" ) predicted_variance [ y_var_negative ] = 0 return predicted_y , predicted_variance else : return predicted_y","title":"predict()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.score","text":"Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values Source code in btjenesten/gpr.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def score ( self , input_data_X , input_data_Y ): \"\"\" Returns the average and maximum error of our predict method. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- input_data_X: input data that the gpr will predict corresponding output data to. input_data_Y: Corresponding true ouput data for input_data_X. Returns: -------- avg_error - the average error between the predicted values and the true values max_error - the maximum error between the predicted values and the true values \"\"\" predicted_y = self . predict ( input_data_X ) avg_error = np . mean ( np . abs ( predicted_y - input_data_Y )) max_error = np . max ( np . abs ( predicted_y - input_data_Y )) return avg_error , max_error","title":"score()"},{"location":"gpr-reference/#btjenesten.gpr.Regressor.update","text":"Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan ceadyy@gmail.com new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. Source code in btjenesten/gpr.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update ( self , new_X , new_Y , tol = 1e-5 ): \"\"\" Updates the training data in accordance to some newly measured data. Author: Christian Elias Anderssen Dalan <ceadyy@gmail.com> Parameters: ----------- new_X: Set of new features that have been measured. new_Y: Corresponding set of labels to new_X. tol: Tolerance which the training data set can differ from new points. If this is too low you may encounter singular covariance matrices. \"\"\" assert type ( new_Y ) is np . ndarray , \"Data error!!!!! Needs to be array.\" assert type ( new_X ) is np . ndarray , \"Data error!!!!! Needs to be array.\" for measurement in new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]): for i in range ( len ( self . training_data_X )): if np . allclose ( measurement , self . training_data_X [ i ], atol = tol ): print ( f \"The model has most likely converged! { measurement } already exists in the training set.\" ) return True \"\"\" old_X_shape = self.training_data_X.shape old_Y_shape = len(self.training_data_Y) new_X_shape = np.array(self.training_data_X.shape) new_Y_shape = len(new_Y) new_X_shape[0] += new_X.shape[0] new_Y_shape += len(new_Y) new_training_data_X = np.zeros(new_X_shape) new_training_data_Y = np.zeros(new_Y_shape) new_training_data_X[:-old_X_shape.shape[0]] = self.training_data_X new_training_data_X[-new_X.shape[0]:] = new_X new_training_data_Y[:-old_Y_shape] = self.training_data_Y new_training_data_Y[-new_Y_shape:] = new_Y \"\"\" #print(\"X1 shape \",self.training_data_X.shape) #print(\"X2 shape \",.shape) new_X = new_X . reshape ( - 1 , self . training_data_X . shape [ 1 ]) new_training_data_X = np . concatenate (( self . training_data_X , self . normalize ( new_X ))) new_training_data_Y = np . concatenate (( self . training_data_Y , new_Y / self . normalization_factor_y )) #indexes = np.argsort(new_training_data_X) self . training_data_X = new_training_data_X #[indexes] self . training_data_Y = new_training_data_Y #[indexes] return False","title":"update()"},{"location":"gpr-reference/#btjenesten.gpr.no_normalization","text":"generate functions which essentially does nothing Author: No-one Source code in btjenesten/gpr.py 374 375 376 377 378 379 380 381 382 383 384 385 386 def no_normalization ( training_data ): \"\"\" generate functions which essentially does nothing Author: No-one \"\"\" def normalize ( training_data ): return training_data def recover ( training_data ): return training_data return normalize , recover","title":"no_normalization()"},{"location":"gpr-reference/#btjenesten.gpr.normalize_training_data_x","text":"generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) Source code in btjenesten/gpr.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def normalize_training_data_x ( training_data ): \"\"\" generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) \"\"\" mean = np . mean ( training_data , axis = 0 ) bound = np . max ( training_data , axis = 0 ) - np . min ( training_data , axis = 0 ) def normalize ( training_data , mean = mean , bound = bound ): training_data_normalized = training_data - mean [ None ,:] training_data_normalized *= ( .5 * bound [ None , :]) **- 1 return training_data_normalized def recover ( training_data_normalized , mean = mean , bound = bound ): return training_data_normalized * ( .5 * bound [ None ,:]) + mean [ None , :] return normalize , recover","title":"normalize_training_data_x()"},{"location":"gpr-reference/#btjenesten.gpr.normalize_training_data_x_log","text":"generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) Source code in btjenesten/gpr.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def normalize_training_data_x_log ( training_data ): \"\"\" generate functions to normalize and recover unnormalized training data Author: Audun (more detailed explanation is required) \"\"\" #mean = np.mean(training_data, axis =0 ) min_ = np . min ( training_data , axis = 0 ) - 1e-3 bound = np . max ( training_data , axis = 0 ) - np . min ( training_data , axis = 0 ) def normalize ( training_data , min_ = min_ , bound = bound ): training_data_normalized = training_data return np . log ( training_data_normalized ) def recover ( training_data_normalized , min_ = min_ , bound = bound ): return np . exp ( training_data_normalized ) return normalize , recover","title":"normalize_training_data_x_log()"},{"location":"gpr-reference/#btjenesten.kernels.Constant","text":"Kernel that returns a constant covariance value between \\(x_i\\) and \\(x_j\\) Useful if all values depend on eachother equally. X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. A matrix with the same shape as our input data. All matrix elements have the value k. Source code in btjenesten/kernels.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def Constant ( X1 , X2 , k = 0.5 ): \"\"\" Kernel that returns a constant covariance value between $x_i$ and $x_j$ Useful if all values depend on eachother equally. Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the covariance. Returns: A matrix with the same shape as our input data. All matrix elements have the value k. \"\"\" return np . ones ( X1 . shape ) * k","title":"Constant()"},{"location":"gpr-reference/#btjenesten.kernels.Funny_trigonometric","text":"Kernel that I made only for fun. May work for extravagant datasets X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. A covariance matrix that might be a bit crazy. Source code in btjenesten/kernels.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def Funny_trigonometric ( X1 , X2 , k = 1 ): \"\"\" Kernel that I made only for fun. May work for extravagant datasets Parameters: ----------- X1: Dataset 1 X2: Dataset 2 k: constant that determines the frequency of the trigonometric functions. Returns: A covariance matrix that might be a bit crazy. \"\"\" return np . sin ( - k * ( x [:, None ] - y [ None ,:]) ** 2 ) - np . cos ( - k * ( x [:, None ] - y [ None ,:]) ** 2 )","title":"Funny_trigonometric()"},{"location":"gpr-reference/#btjenesten.kernels.RBF","text":"Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between \\(x_i\\) and \\(x_j\\) . Increasing l will decrease the covariance, and vice versa. A matrix with the same shape as our input data, where the elemets are: \\(e^{-l \\cdot d(x_i, x_j)}\\) where \\(d(x_i, x_j)\\) is the difference between element \\(x_i\\) in X1 and element \\(x_j\\) in X2. Source code in btjenesten/kernels.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def RBF ( X1 , X2 , l = np . array ([ 1.0 ])): \"\"\" Radial basis function of the form: $$ e^{-l * d(X_i,X_j)} $$ Parameters: ----------- X1: Dataset 1 X2: Dataset 2 l: Length scale parameter. Can be adjusted to adjust the covariance between $x_i$ and $x_j$. Increasing l will decrease the covariance, and vice versa. Returns: ----------- A matrix with the same shape as our input data, where the elemets are: $e^{-l \\cdot d(x_i, x_j)}$ where $d(x_i, x_j)$ is the difference between element $x_i$ in X1 and element $x_j$ in X2. \"\"\" if type ( l ) is not np . ndarray : # patch for scalar length parameter l = np . array ([ l ]) ld = np . sum ( l [ None , :] * ( X1 . reshape ( X1 . shape [ 0 ], - 1 )[:, None ] - X2 . reshape ( X2 . shape [ 0 ], - 1 )[ None ,]) ** 2 , axis = 2 ) return np . exp ( - ld )","title":"RBF()"},{"location":"veiledere/","text":"Veilederne i B-tjenesten f\u00e5r pedagogisk oppl\u00e6ring gjennom L\u00e6ringsassistentprogrammet ved Universitetet i Oslo, og f\u00e5r verdifull praktisk erfaring med utviklerverkt\u00f8y som git, markdown og Python. Tidligere veiledere er markert i gr\u00e5tt. .card { box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2); max-width: 180px; margin: 5px 5px 5px 5px; padding: 25px; text-align: center; float: left; } .card2 { box-shadow: 0 4px 8px 0 rgba(0.0, 0, 0.0, 0.2); max-width: 180px; margin: 5px 5px 5px 5px; padding: 25px; text-align: center; float: left; background-color: rgba(0, 0, 0, 0.1) } .float-container { border: 3px solid #fff; padding: 20px; } .title { color: grey; font-size: 18px; } button { border: none; outline: 0; display: inline-block; padding: 8px; color: white; background-color: #000; text-align: center; cursor: pointer; width: 100%; font-size: 18px; } a { text-decoration: none; font-size: 22px; color: black; } button:hover, a:hover { opacity: 0.7; } Hanan Gharayba Ayla S. Coder Christine Hansen Dawid Szefler Elias A. Dalan","title":"V\u00e5re veiledere"}]}